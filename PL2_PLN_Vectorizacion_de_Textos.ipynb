{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceRb6/Procesamiento_de_Lenguaje_Natural/blob/main/PL2_PLN_Vectorizacion_de_Textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c934b07",
      "metadata": {
        "id": "7c934b07"
      },
      "source": [
        "# Practica 2: Vectorizacion de textos\n",
        "\n",
        "Para estudiar y practicar el Modelo de Espacio Vectorial (VSM) y el Modelo de Bolsa de Palabras (BoW) en Procesamiento de Lenguaje Natural (PLN) nos enfocaremos  en el conjunto de datos de sklear 20 Newsgroups que contiene publicaciones de 20 grupos de noticias diferentes.\n",
        "\n",
        "Deberan implementar el cálculo manual de la Similitud del Coseno, Producto Punto y distancia euclidea en el dataset 20 Newsgroups y analizarán cómo varían las similitudes dentro y entre categorías.\n",
        "\n",
        "- Tokenización manual\n",
        "- Normalización\n",
        "- Stopwords y lematización con librerías\n",
        "- Construcción manual de Bag of Words (BoW)\n",
        "- Cálculo manual de la Similitud del Coseno"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1801574d",
      "metadata": {
        "id": "1801574d"
      },
      "source": [
        "## 1. Cargar el Dataset\n",
        "\n",
        "Para la practica de hoy utilizaremos el dataset 20-Newsgroups de la libreria sklearn que contiene publicaciones de 20 grupos de noticias diferentes:\n",
        "\n",
        "Primero deberán cargar y eligir alguna(s) categorias del dataset:\n",
        "\n",
        "['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f72234",
      "metadata": {
        "id": "46f72234"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55170429",
      "metadata": {
        "id": "55170429"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Carga del dataset\n",
        "categorias_seleccionadas = ['sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categorias_seleccionadas, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745c4cac",
      "metadata": {
        "id": "745c4cac",
        "outputId": "91fe1fd7-3791-4dce-b532-d453c8e08e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de documentos: ['sci.space']\n",
            "Total de documentos: 593\n",
            "\n",
            "Ejemplo:\n",
            " \n",
            "Any lunar satellite needs fuel to do regular orbit corrections, and when\n",
            "its fuel runs out it will crash within months.  The orbits of the Apollo\n",
            "motherships changed noticeably during lunar missions lasting only a few\n",
            "days.  It is *possible* that there are stable orbits here and there --\n",
            "the Moon's gravitational field is poorly mapped -- but we know of none.\n",
            "\n",
            "Perturbations from Sun and Earth are relatively minor issues at low\n",
            "altitudes.  The big problem is that the Moon's own gravitational field\n",
            "is quite lumpy due to the irregular distribution of mass within the Moon.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total de documentos: {newsgroups.target_names}\")\n",
        "print(f\"Total de documentos: {len(newsgroups.data)}\")\n",
        "print(f\"\\nEjemplo:\\n {newsgroups.data[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce45e130",
      "metadata": {
        "id": "ce45e130"
      },
      "source": [
        "### 1.1 Analisis Exploratorio\n",
        "\n",
        "En este punto deben explorar y analizar el contenido del dataset antes de empezar la tokenización, normalizacion, vectorizacion y definir el proceso de similitud.\n",
        "\n",
        "Las tareas que deben completar son:\n",
        "\n",
        "- Analizar el número de documentos por categoría: Deberán contar cuántos documentos hay en cada categoría.\n",
        "- Explorar la longitud de los textos: Deberán analizar el numero de palabras en los diferentes textos (Máximo, Mínimo, Promedio).\n",
        "- Revisión de Documentos Aleatorios: Investiguen documentos aleatorios y definan si el lenguaje es formal o informal y si hay palabras características de la categoria (s) que escogieron."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8ccdef",
      "metadata": {
        "id": "2c8ccdef"
      },
      "source": [
        "### Evaluación\n",
        "\n",
        "1. ¿Qué diferencias notaste entre las categorías?\n",
        "2. ¿Cómo esperas que sea la similitud entre documentos de la misma categoría?\n",
        "3. ¿Cómo crees que afectará el preprocesamiento (eliminar palabras, normalizar texto) los resultados de similitud?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0575238",
      "metadata": {
        "id": "e0575238"
      },
      "source": [
        "## 2. Tokenización y Normalización\n",
        "\n",
        "En este punto deben seleccionar la mejor forma de tokenizar los textos, que  técnicas de normalización utilizarán (stopwords, lematización, stemming) y justificar su elección según las características del conjunto de datos.\n",
        "\n",
        "- NO usar sklearn ni CountVectorizer para tokenización.\n",
        "- Pueden usar nltk y spacy para normalización."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b1d2d7",
      "metadata": {
        "id": "68b1d2d7"
      },
      "source": [
        "1. Eliminación de stopwords:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21f4155",
      "metadata": {
        "id": "c21f4155"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def eliminar_stopwords(tokens):\n",
        "    return [palabra for palabra in tokens if palabra not in stop_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce20cd06",
      "metadata": {
        "id": "ce20cd06"
      },
      "source": [
        "2. Recortar palabras a su raiz si aplica (Steemming):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd75049",
      "metadata": {
        "id": "ddd75049"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(tokens):\n",
        "    return [stemmer.stem(palabra) for palabra in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e782be3e",
      "metadata": {
        "id": "e782be3e"
      },
      "source": [
        "3. Convertir palabras a su forma base (Lematización)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4bc868",
      "metadata": {
        "id": "3f4bc868"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(palabra) for palabra in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40223df",
      "metadata": {
        "id": "d40223df"
      },
      "source": [
        "### Evaluación\n",
        "\n",
        "1. ¿Qué técnica de tokenización eligieron y por qué?\n",
        "2. ¿Qué técnicas de normalización usaron y cómo esperan que impacten el entendimiento del texto?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d668bc2c",
      "metadata": {
        "id": "d668bc2c"
      },
      "source": [
        "## 3. Explorando y Construyendo Bolsa de Palabras (Bag of Words (BoW))\n",
        "\n",
        "En este punto deberán generar una matriz de Bolsa de Palabras usando el enfoque binario (asignar 0 o 1 si la palabra existe en un documento) o simplemente contando las ocurrencias de una palabra del vocabulario en los textos.\n",
        "\n",
        "1. Construcción del vocabulario. Del conjunto de tokens que generaron deberán construir un vocabulario de terminos.\n",
        "\n",
        "- ¿Deberiamos limitar el numero de palabras?\n",
        "- ¿Eliminar manualmente palabras que no aportan significado?\n",
        "- ¿Cómo crees que cambiaría BoW si usamos todas las palabras posibles del vocabulario?\n",
        "\n",
        "2. Construcción de la Matriz BoW. En este punto representamos cada documento como un vector de frecuencia basado en el vocabulario.\n",
        "\n",
        "- ¿Cuán dispersa es la matriz BoW? (Muchos ceros vs. valores útiles)\n",
        "\n",
        "- ¿Cómo podríamos reducir la dispersión de la matriz sin perder información relevante?\n",
        "\n",
        "3. Análisis de la Frecuencia de Palabras. En este punto basta con analizar la distribución de lsa palabras obteniedo aquellas que parecen más comunes.\n",
        "\n",
        "- ¿Las palabras más frecuentes aportan información útil o son triviales?\n",
        "- ¿Deberíamos filtrar palabras extremadamente frecuentes o poco frecuentes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad8ce06",
      "metadata": {
        "id": "1ad8ce06"
      },
      "source": [
        "## 4. Cálculo de la Similitud entre documentos\n",
        "\n",
        "En este punto deben implementar una función para calcular la Similitud usando al menos 2 metricas. Ademásdeberan comparar documentos dentro y fuera de la misma categoría(si utlizaron 2 o mas categorias).\n",
        "\n",
        "Pueden usar numpy para calcular el producto punto y la norma de los vectores:\n",
        "\n",
        " - np.dot(a, b)\n",
        " - np.linalg.norm(a)\n",
        " - np.linalg.norm(b)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465c0b00",
      "metadata": {
        "id": "465c0b00"
      },
      "source": [
        "Ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf4627a",
      "metadata": {
        "id": "cdf4627a"
      },
      "outputs": [],
      "source": [
        "def similitud_producto_punto(vec1, vec2):\n",
        "    '''\n",
        "    logica aqui\n",
        "    '''\n",
        "    return  dot_product\n",
        "\n",
        "def similitud_coseno(vec1, vec2):\n",
        "    '''\n",
        "    logica aqui\n",
        "    '''\n",
        "    return  dot_product / (norma_vec1 * norma_vec2)\n",
        "\n",
        "doc1 = matriz_bow[0]\n",
        "doc2 = matriz_bow[1]\n",
        "\n",
        "sim = similitud_coseno(doc1, doc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56605d1d",
      "metadata": {
        "id": "56605d1d"
      },
      "source": [
        "Evaluación:\n",
        "\n",
        "- ¿Los resultados de similitud coinciden con tus expectativas?\n",
        "- ¿Qué patrones encontraste en la similitud entre categorías?\n",
        "- ¿Cómo podríamos mejorar la calidad de las similitudes?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}